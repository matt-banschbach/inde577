{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bagging and Random Forest\n",
    "\n",
    "In this notebook, we begin by implementing a Bootstrap-Aggregating classifier using many stump classifiers. Recognizing that this is technically a random forest, then utilize the built-in RandomForestClassifier, which is slightly more robust due to the randomness it introduces in sampling. \n",
    "\n",
    "\n",
    "### Random Forest Overview\n",
    "\n",
    "Random forests are a type of ensemble learning method used in machine learning for both classification and regression \n",
    "tasks. Random forests are built on the principle of combining multiple decision trees to improve the accuracy and \n",
    "robustness of predictions. Here are the key steps involved:\n",
    "\n",
    "1. Bootstrap Sampling: Random forests start by creating multiple bootstrap \n",
    "samples from the original dataset. Each sample is obtained by randomly selecting data points with \n",
    "replacement, a process known as bagging.\n",
    "\n",
    "2. Decision Tree Construction: For each bootstrap sample, a decision tree is constructed. However, unlike \n",
    "traditional decision trees, random forests introduce randomness during the tree-building process. \n",
    "At each node, instead of considering all features, a random subset of features is selected for splitting. \n",
    "This is known as feature randomness or the random subspace method.\n",
    "\n",
    "3. Combining Predictions: For classification tasks, each decision tree in the forest gives a classification or a \"vote,\" \n",
    "and the final prediction is the class with the majority of the votes. For regression tasks, the final prediction is\n",
    "the average of the outputs of all trees.\n",
    "\n",
    "Underlying Math\n",
    "The underlying mathematics of random forests involves several key concepts:\n",
    "\n",
    "1. Variance Reduction: Random forests reduce the variance of the predictions by averaging the outputs of multiple \n",
    "uncorrelated decision trees. This is based on the principle that the variance of the average of independent random \n",
    "variables is less than the variance of any individual variable.\n",
    "\n",
    "2. Correlation Reduction: By randomly selecting features at each split, random forests reduce the correlation between \n",
    "the decision trees. This is crucial because the benefits of averaging (bagging) are limited by the correlation \n",
    "between the trees. Reducing correlation enhances the variance reduction achieved through bagging.\n",
    "\n",
    "3. Feature Importance: Random forests measure the importance of each feature by evaluating the impact of each feature \n",
    "on the modelâ€™s performance. This is often done through permutation tests, where the importance of a feature is \n",
    "calculated by measuring the decrease in model performance when the values of that feature are randomly permuted.\n",
    "\n",
    "Benefits of Random Forests:\n",
    "\n",
    "1. High Accuracy: By combining multiple decision trees, random forests achieve higher accuracy than individual decision trees. \n",
    "This ensemble approach captures a broader range of data patterns, leading to more precise predictions.\n",
    "\n",
    "2. Robustness to Overfitting: Random forests are highly resistant to overfitting due to the randomness in bootstrapping \n",
    "and feature selection. This ensures that the model generalizes well to new, unseen data.\n",
    "\n",
    "\n",
    "3. Handling Missing Data: Random forests can handle missing values naturally by using the split with the majority \n",
    "of the data and averaging the outputs from trees trained on different parts of the data.\n",
    "\n",
    "4. Feature Importance: Random forests provide insights into which features are most influential \n",
    "in making predictions, which is valuable for understanding the data and selecting relevant features.\n",
    "\n",
    "5. Versatility: Random forests are effective for both classification and regression tasks and can \n",
    "handle high-dimensional datasets with ease.\n",
    "\n",
    "Limitations of Random Forests\n",
    "\n",
    "1. Computational Cost: Training multiple decision trees can be computationally expensive, especially with large datasets \n",
    "and a high number of trees. This increases memory usage and training times.\n",
    "\n",
    "2. Interpretability: While individual decision trees are easy to interpret, random forests, being an ensemble \n",
    "of many trees, are more complex and harder to interpret. This lack of transparency can be a disadvantage in \n",
    "situations where model interpretability is crucial.\n",
    "\n",
    "3. Prediction Time: Random forest models can take longer to make predictions compared to other algorithms, \n",
    "which can be a drawback in real-time applications.\n",
    "\n",
    "4. Parameter Tuning: The performance of random forests heavily depends on various hyperparameters such as \n",
    "the number of trees, the maximum depth of each tree, and the number of features considered at each split. \n",
    "Proper tuning of these parameters is essential but can be time-consuming and requires expertise.\n",
    "\n",
    "5. Noise Sensitivity: Random forests can struggle with datasets that have high levels of noise. \n",
    "The algorithm may construct trees that overfit the noise, reducing overall model accuracy.\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "This notebook uses the wine dataset from sklearn.\n",
    "\n",
    "#### Reproducibility\n",
    "\n",
    "Ensure all random states are set to 42"
   ],
   "id": "865f08dc15581c37"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-17T01:01:14.787883Z",
     "start_time": "2024-12-17T01:01:13.630974Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "from em_el.datasets import load_wine"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:02:36.305100Z",
     "start_time": "2024-12-17T01:02:36.275241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wine = load_wine()\n",
    "X = wine.drop('target', axis=1).to_numpy()\n",
    "y = wine['target'].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "c33c29532e462293",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:13:27.069383Z",
     "start_time": "2024-12-17T01:13:27.003334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=1, random_state=42), random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "print(\"Classification Report - Bagging: \\n\", classification_report(y_test, bag_y_pred))"
   ],
   "id": "1980473fc4883fd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report - Bagging: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.84        14\n",
      "           1       0.92      0.79      0.85        14\n",
      "           2       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.89      0.86      0.87        36\n",
      "weighted avg       0.88      0.86      0.86        36\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:13:17.671022Z",
     "start_time": "2024-12-17T01:13:17.645534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# How does this compare to a normal decision tree?\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth = 10, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)\n",
    "tree_clf_rep = classification_report(y_test, bag_y_pred)\n",
    "print(\"Classification Report - Decision Tree: \\n\", tree_clf_rep)"
   ],
   "id": "802430c12d8d2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report - Decision Tree: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.84        14\n",
      "           1       0.92      0.79      0.85        14\n",
      "           2       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.89      0.86      0.87        36\n",
      "weighted avg       0.88      0.86      0.86        36\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:13:32.926913Z",
     "start_time": "2024-12-17T01:13:32.669283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "rf_clf_rep = classification_report(y_test, rf_y_pred)\n",
    "print(f\"Random Forest Classification Report: \\n\", rf_clf_rep)"
   ],
   "id": "d0dc7ce9f0164d1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this case, the random forest outperforms the decision tree without any hyperparameter tuning. It's also worth noting that using the Built-in Random Forest algorithm in sklearn is generally superior to using Bagging with stumps, because the built-in algorithm incorporates additional randomness that leads to greater variance reduction.",
   "id": "e992bfc6327b6093"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:07:42.682192Z",
     "start_time": "2024-12-17T01:07:42.658799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Feature Importances\")\n",
    "list(zip(list(wine.drop('target', axis=1).columns), rf_clf.feature_importances_))"
   ],
   "id": "fc8cc51e6f92b776",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('alcohol', 0.11239773542143086),\n",
       " ('malic_acid', 0.03570276433546083),\n",
       " ('ash', 0.021282064154184602),\n",
       " ('alcalinity_of_ash', 0.03242487609714125),\n",
       " ('magnesium', 0.03684069949458186),\n",
       " ('total_phenols', 0.029278585609125395),\n",
       " ('flavanoids', 0.20229341635663622),\n",
       " ('nonflavanoid_phenols', 0.013515250584037197),\n",
       " ('proanthocyanins', 0.023560915987205423),\n",
       " ('color_intensity', 0.1712021830864957),\n",
       " ('hue', 0.07089132259413944),\n",
       " ('od280/od315_of_diluted_wines', 0.1115643167260497),\n",
       " ('proline', 0.13904586955351153)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "While the feature importances are relatively uniform, the algorithm deemed flavanoids, color_intensity, and procline to be the most important, these three summing to just over 50% of total importance.",
   "id": "27e1ffffff1d370f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71c83595e09b9a59"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
