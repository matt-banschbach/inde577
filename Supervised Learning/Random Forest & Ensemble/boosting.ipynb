{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Boosting\n",
    "___\n",
    "\n",
    "### Overview\n",
    "\n",
    "Boosting is an ensemble learning technique in supervised machine learning that combines multiple weak learners to create a strong learner, improving predictive accuracy and reducing errors. AdaBoost (Adaptive Boosting) is one of the most popular boosting algorithms, developed by Yoav Freund and Robert Schapire in 1995.\n",
    "\n",
    "## Underlying Mathematics\n",
    "\n",
    "AdaBoost works by iteratively training weak classifiers (often decision stumps) and adjusting sample weights based on classification errors. The key mathematical concepts include:\n",
    "\n",
    "1. Sample Weighting: Initially, all samples have equal weights:\n",
    "\n",
    "   $$w_i = \\frac{1}{N}$$\n",
    "\n",
    "   where N is the total number of data points.\n",
    "\n",
    "2. Error Calculation: For each weak classifier, calculate the weighted error:\n",
    "\n",
    "   $$\\epsilon_m = \\frac{\\sum_{y_i \\neq k_m(x_i)} w_i^{(m)}}{\\sum_{i=1}^N w_i^{(m)}}$$\n",
    "\n",
    "   where $k_m(x_i)$ is the prediction of the m-th classifier for sample $x_i$.\n",
    "\n",
    "3. Classifier Weight: Determine the importance of each classifier:\n",
    "\n",
    "   $$\\alpha_m = \\frac{1}{2} \\ln\\left(\\frac{1-\\epsilon_m}{\\epsilon_m}\\right)$$\n",
    "\n",
    "4. Weight Update: Adjust sample weights for the next iteration:\n",
    "\n",
    "   $$w_i^{(m+1)} = w_i^{(m)} \\exp(-\\alpha_m y_i k_m(x_i))$$\n",
    "\n",
    "5. Final Classifier: The strong classifier is a weighted combination of weak classifiers:\n",
    "\n",
    "   $$F(x) = \\text{sign}\\left(\\sum_{m=1}^M \\alpha_m k_m(x)\\right)$$\n",
    "\n",
    "##### Benefits\n",
    "\n",
    "1. High Accuracy: AdaBoost can achieve high prediction accuracy by combining weak learners.\n",
    "2. Reduced Bias: It effectively reduces bias in machine learning models.\n",
    "3. Ease of Implementation: AdaBoost is relatively easy to implement and interpret.\n",
    "4. Versatility: It can be used with various types of weak learners and for both classification and regression tasks.\n",
    "5. Automatic Feature Selection: AdaBoost inherently performs feature selection by focusing on the most informative features.\n",
    "\n",
    "##### Limitations\n",
    "\n",
    "1. Sensitivity to Noisy Data: AdaBoost is highly sensitive to noisy data and outliers, which can lead to overfitting.\n",
    "2. Computational Cost: It can be computationally expensive, especially for large datasets or complex base learners.\n",
    "3. Sequential Nature: The algorithm's sequential nature makes it difficult to parallelize, limiting scalability for large datasets.\n",
    "4. Potential for Overfitting: While generally resistant to overfitting, it can still occur, especially with noisy datasets.\n",
    "5. Slower Than Some Alternatives: AdaBoost can be slower compared to other boosting algorithms like XGBoost.\n"
   ],
   "id": "173d56e5ddf47137"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-16T22:10:05.402133Z",
     "start_time": "2024-12-16T22:10:03.994766Z"
    }
   },
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from em_el.datasets import load_wine\n",
    "from em_el.utils import draw_confusion_matrix"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:10:43.408253Z",
     "start_time": "2024-12-16T22:10:43.381976Z"
    }
   },
   "cell_type": "code",
   "source": "wine = load_wine()",
   "id": "36fa23d45a2e498e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:14:30.160556Z",
     "start_time": "2024-12-16T22:14:30.150402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = wine.drop('target', axis=1).to_numpy()\n",
    "y = wine['target'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "67684154bd42726d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:11:01.358315Z",
     "start_time": "2024-12-16T22:11:01.146659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), random_state=42, n_estimators = 50,\n",
    "                             algorithm = \"SAMME\",\n",
    "                             learning_rate = 0.5)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_y_pred = ada_clf.predict(X_test)"
   ],
   "id": "353cb09f62d1e5f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:11:03.176299Z",
     "start_time": "2024-12-16T22:11:03.153812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ada_acc = accuracy_score(y_test, ada_y_pred)\n",
    "ada_clf_rep = classification_report(y_test, ada_y_pred)\n",
    "\n",
    "print(\"Ada Accuracy: \\n\", ada_acc)\n",
    "print(\"Ada Classification Report: \\n\", ada_clf_rep)"
   ],
   "id": "1a8e05f7f47c7ff3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Accuracy: \n",
      " 0.9166666666666666\n",
      "Ada Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97        14\n",
      "           1       0.92      0.86      0.89        14\n",
      "           2       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.91      0.91      0.91        36\n",
      "weighted avg       0.92      0.92      0.92        36\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:11:08.383387Z",
     "start_time": "2024-12-16T22:11:08.371616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compare with Single Decision Tree\n",
    "tree_clf = DecisionTreeClassifier(max_depth=15, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)"
   ],
   "id": "31c3f3282ba5490b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:11:11.605808Z",
     "start_time": "2024-12-16T22:11:11.576975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tree_acc = accuracy_score(y_test, tree_y_pred)\n",
    "tree_clf_rep = classification_report(y_test, tree_y_pred)\n",
    "\n",
    "print(\"Decision Tree Accuracy: \\n\", tree_acc)\n",
    "print(\"Decision Tree Classification Report: \\n\", tree_clf_rep)"
   ],
   "id": "59062620c4dbba3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: \n",
      " 0.9444444444444444\n",
      "Decision Tree Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        14\n",
      "           1       0.93      1.00      0.97        14\n",
      "           2       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.95      0.93      0.94        36\n",
      "weighted avg       0.95      0.94      0.94        36\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With these starting parameters, the AdaBoost classifier is not outperforming the single tree. Below, we conduct hyperparameter tuning on the AdaBoost classifier to try and improve performance",
   "id": "a6447630cb58d28c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:12:40.411448Z",
     "start_time": "2024-12-16T22:12:07.863371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameter Tuning with GridSearch\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.1, 0.5, 1.0],\n",
    "    'estimator__max_depth': [1, 3, 5],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Initialize the AdaBoost classifier\n",
    "ada_boost = AdaBoostClassifier(base_estimator, algorithm = \"SAMME\", random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=ada_boost, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best parameters and the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ],
   "id": "454bb1e7185367e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'estimator__max_depth': 5, 'estimator__min_samples_leaf': 10, 'estimator__min_samples_split': 2, 'learning_rate': 1.0, 'n_estimators': 50}\n",
      "Best Accuracy: 0.9858156028368795\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:14:35.389734Z",
     "start_time": "2024-12-16T22:14:35.180036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5, min_samples_leaf=10, min_samples_split=2), random_state=42, n_estimators=50, algorithm=\"SAMME\", learning_rate=1.0)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_y_pred = ada_clf.predict(X_test)\n",
    "\n",
    "ada_acc = accuracy_score(y_test, ada_y_pred)\n",
    "ada_clf_rep = classification_report(y_test, ada_y_pred)\n",
    "\n",
    "print(\"Ada Accuracy: \\n\", ada_acc)\n",
    "print(\"Ada Classification Report: \\n\", ada_clf_rep)"
   ],
   "id": "fa4e1b95bbf1f0d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Accuracy: \n",
      " 1.0\n",
      "Ada Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With Hyperparameter Tuning, our results are improved considerably",
   "id": "433a3dde89155cab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "43a0c2db671a013",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
