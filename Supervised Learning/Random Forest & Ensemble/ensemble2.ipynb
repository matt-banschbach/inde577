{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "Here, we demonstrate the use of both Voting and Random Forests (two ensemble methods), as well as a standard decision tree and a support vector machine (SVM). Below we provide two overviews for the methods, voting and SVMs.\n",
    "\n",
    "### SVM Overview\n",
    "___\n",
    "Support Vector Machines (SVMs) are powerful supervised learning algorithms used for classification and regression tasks in machine learning. They work by finding the optimal hyperplane that separates different classes of data points with the maximum margin.\n",
    "\n",
    "##### Underlying Mathematics\n",
    "\n",
    "The key mathematical concepts behind SVMs include:\n",
    "\n",
    "1. Hyperplane: In an n-dimensional space, it's defined as:\n",
    "\n",
    "   $$w^T x + b = 0$$\n",
    "\n",
    "   where $w$ is the weight vector, $x$ is the input vector, and $b$ is the bias.\n",
    "\n",
    "2. Margin Maximization: SVMs aim to maximize the margin between classes, which is calculated as:\n",
    "\n",
    "   $$\\text{Margin} = \\frac{2}{\\|w\\|}$$\n",
    "\n",
    "3. Optimization Problem: The SVM training process solves the following optimization problem:\n",
    "\n",
    "   $$\\min_{w,b} \\frac{1}{2}\\|w\\|^2$$\n",
    "   $$\\text{subject to } y_i(w^T x_i + b) \\geq 1, \\forall i$$\n",
    "\n",
    "   where y_i are the class labels.\n",
    "\n",
    "4. Kernel Trick: For non-linear separation, SVMs use kernel functions to transform the input space:\n",
    "\n",
    "   $$K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)$$\n",
    "\n",
    "   Common kernels include RBF (Radial Basis Function) and polynomial kernels.\n",
    "\n",
    "##### Benefits\n",
    "\n",
    "1. Effective in high-dimensional spaces.\n",
    "2. Memory-efficient, as they use only a subset of training points (support vectors).\n",
    "3. Versatile through different kernel functions.\n",
    "4. Works well with small datasets.\n",
    "5. Less sensitive to noise and outliers.\n",
    "\n",
    "##### Limitations\n",
    "\n",
    "1. Sensitive to parameter selection, requiring careful tuning.\n",
    "2. High algorithmic complexity and memory intensive for large datasets.\n",
    "3. Poor performance with highly imbalanced data.\n",
    "4. Difficulty handling noisy data effectively.\n",
    "5. Limited scalability to very large datasets due to quadratic to cubic time complexity.\n",
    "6. Lack of probabilistic explanation for classifications.\n",
    "\n",
    "SVMs are particularly effective for binary classification problems and can be extended to multi-class classification. Their ability to handle non-linear data through kernel tricks makes them versatile for various applications, including image classification, text categorization, and bioinformatics.\n",
    "___\n",
    "\n",
    "### Voting Overview\n",
    "___\n",
    "\n",
    "Voting ensemble is a popular technique in supervised learning that combines predictions from multiple individual models to make a final prediction. It is primarily used for classification tasks, though it can be adapted for regression as well.\n",
    "\n",
    "##### Underlying Mathematics\n",
    "\n",
    "There are two main types of voting:\n",
    "\n",
    "1. Hard Voting (Majority Voting):\n",
    "   Each model casts a vote for a class, and the class with the most votes wins. Mathematically, for N models:\n",
    "\n",
    "   $$\\hat{y} = \\text{mode}(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_N)$$\n",
    "\n",
    "   where $\\hat{y}$ is the final prediction and $\\hat{y}_i$ is the prediction of the i-th model.\n",
    "\n",
    "2. Soft Voting:\n",
    "   Models provide probability estimates for each class. The final prediction is based on the average probabilities:\n",
    "\n",
    "   $$P(y=j) = \\frac{1}{N} \\sum_{i=1}^N P_i(y=j)$$\n",
    "\n",
    "   where $P(y=j)$ is the probability of class j, and $P_i(y=j)$ is the probability estimate from the i-th model.\n",
    "\n",
    "##### Benefits\n",
    "\n",
    "1. Improved Accuracy: Voting ensembles often achieve higher predictive accuracy than individual models.\n",
    "2. Robustness: They are more robust to outliers and noisy data, reducing the impact of individual model errors.\n",
    "3. Generalization: Ensemble models tend to generalize better to new, unseen data.\n",
    "4. Versatility: Voting can combine different types of models, leveraging their complementary strengths.\n",
    "5. Reduced Overfitting: By balancing bias and variance, voting ensembles can mitigate overfitting risks.\n",
    "\n",
    "##### Limitations\n",
    "\n",
    "1. Increased Complexity: Voting ensembles are computationally expensive and time-consuming due to training and storing multiple models.\n",
    "2. Interpretability Challenges: The combined model is often less interpretable than individual models, making it difficult to explain predictions.\n",
    "3. Dependency on Base Models: The effectiveness of voting ensembles relies heavily on the quality and diversity of the base models.\n",
    "4. Risk of Overfitting: If base models are too complex or similar, there's still a risk of overfitting to the training data.\n",
    "5. Increased Resource Requirements: Voting ensembles demand more computational resources and memory compared to single models.\n",
    "___\n"
   ],
   "id": "b99e6d240de98739"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T23:22:19.037203Z",
     "start_time": "2024-12-17T23:22:19.027223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ],
   "id": "a65ffa40582f7ed2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T23:22:41.856216Z",
     "start_time": "2024-12-17T23:22:41.827285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "penguins = sns.load_dataset('penguins').dropna()\n",
    "X = penguins.drop(['island', 'species', 'sex', 'flipper_length_mm'], axis=1)\n",
    "y = penguins['sex']"
   ],
   "id": "21e73a22aee939e6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T23:22:42.772826Z",
     "start_time": "2024-12-17T23:22:42.757799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "svm_clf = SVC()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "voting_clf = VotingClassifier(estimators=[(\"rf\", rnd_clf), (\"svm\", svm_clf)], voting=\"hard\")"
   ],
   "id": "d5904dc6f0930905",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-17T23:22:45.675224Z",
     "start_time": "2024-12-17T23:22:45.141615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "svm_clf.fit(X_train, y_train)\n",
    "svm_y_predict = svm_clf.predict(X_test)\n",
    "print(f\"svm accuracy: {accuracy_score(y_test, svm_y_predict)}\")\n",
    "\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_predict = tree_clf.predict(X_test)\n",
    "print(f\"tree accuracy: {accuracy_score(y_test, tree_y_predict)}\")\n",
    "\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "rnd_y_predict = rnd_clf.predict(X_test)\n",
    "print(f\"random forest accuracy: {accuracy_score(y_test, rnd_y_predict)}\")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_predict = voting_clf.predict(X_test)\n",
    "print(f\"voting classifier accuracy: {accuracy_score(y_test, y_predict)}\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm accuracy: 0.7238805970149254\n",
      "tree accuracy: 0.8656716417910447\n",
      "random forest accuracy: 0.8880597014925373\n",
      "voting classifier accuracy: 0.8656716417910447\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this case, the random forest classifier has superior performance over the other methods, including the voting classifier, without hyperparameter tuning.",
   "id": "7f67948e779b9249"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
